# Introduction to Spark

The workshop introduces Apache Spark, a platform to process distributed data within the Hadoop environment. The entry threshold to using Spark is fairly low since most of its functionality can be implemented in Python and R programs. In addition, Spark also runs locally on any multi-core desktop or laptop. 

## Sessions
1. Introduction of the core concepts of resilient distributed datasets (RDDs), data-frames, and lazy evaluation.
2. Participants learn how to use the Spark machine-learning library for classification, regression and collaborative filtering.
3. Core concepts of graph analysis and some of the graph algorithms provided by the GraphX library. 

## Resources
1. [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](http://www-bcf.usc.edu/~minlanyu/teach/csci599-fall12/papers/nsdi_spark.pdf)
1. [Apache Spark Documentation](http://spark.apache.org/documentation.html)
2. [Spark - A visual guide to the API](http://training.databricks.com/visualapi.pdf)
3. [Scala Documentation](http://docs.scala-lang.org/index.html)
4. [Compilation of various topics from *databricks* workshop](http://training.databricks.com/workshop/itas_workshop.pdf)

