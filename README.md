# Introduction to Spark

The workshop introduces Apache Spark, a platform to process distributed data within the Hadoop environment. The entry threshold to using Spark is fairly low since most of its functionality can be implemented in Python and R programs. In addition, Spark also runs locally on any multi-core desktop or laptop. 

### Sessions
1. Introduction of the core concepts of resilient distributed datasets (RDDs), data-frames, and lazy evaluation.
2. Participants learn how to use the Spark machine-learning library for classification, regression and collaborative filtering.
3. Core concepts of graph analysis and some of the graph algorithms provided by the GraphX library. 
