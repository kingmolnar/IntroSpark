# Spark Core, RDDs and DataFrames

## Objectives of this session
1. Introduce the concept of RDDs (Resilient Distributed Datasets), and the principle of 'transformations' and 'actions'
2. Run and monitor Spark processes (on the cluster, as well as in other environments)
3. Work with Spark DataFrames in Python, Scala, and R

## Resources
1. [Apache Spark Documentation](http://spark.apache.org/documentation.html)
2. [Spark - A visual guide to the API](http://training.databricks.com/visualapi.pdf)
3. [Scala Documentation](http://docs.scala-lang.org/index.html)

